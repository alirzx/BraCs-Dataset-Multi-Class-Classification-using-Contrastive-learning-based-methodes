{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c503d3a",
   "metadata": {},
   "source": [
    "# Start BraCs Multi class Classification project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d2fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32675c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\New folder\\hub\n",
      "O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\New folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_HOME\"] = r\"O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\New folder\"\n",
    "\n",
    "import torch\n",
    "print(torch.hub.get_dir())\n",
    "print(os.environ[\"TORCH_HOME\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15416558",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591cfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, models\n",
    "from torchvision.models import (\n",
    "    ResNeXt50_32X4D_Weights, DenseNet201_Weights, EfficientNet_B0_Weights,\n",
    "    ResNet18_Weights, ResNet50_Weights, ResNeXt101_32X8D_Weights\n",
    ")\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Step 1: Create DataFrames\n",
    "def create_dataframes(root_dir):\n",
    "    \"\"\"\n",
    "    Create separate DataFrames for train, val, and test splits.\n",
    "\n",
    "    Directory structure:\n",
    "    <root_dir>/\n",
    "        train/\n",
    "            class1/\n",
    "            class2/\n",
    "            ...\n",
    "        val/\n",
    "            class1/\n",
    "            class2/\n",
    "            ...\n",
    "        test/\n",
    "            class1/\n",
    "            class2/\n",
    "            ...\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Root directory of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df, class_to_idx)\n",
    "            - train_df, val_df, test_df: DataFrames with columns \"file_path\", \"label\", \"class_name\"\n",
    "            - class_to_idx: Dict mapping class names to numeric indices\n",
    "    \"\"\"\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    valid_extensions = {'.png', '.jpg', '.jpeg', '.tif', '.bmp'}\n",
    "    data = {split: [] for split in splits}\n",
    "\n",
    "    # Get class names from train split to ensure consistent labeling\n",
    "    train_dir = os.path.join(root_dir, \"train\")\n",
    "    class_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "    for split in splits:\n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "        if not os.path.isdir(split_dir):\n",
    "            print(f\"Warning: {split_dir} does not exist!\")\n",
    "            continue\n",
    "\n",
    "        for class_name in os.listdir(split_dir):\n",
    "            class_dir = os.path.join(split_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                continue\n",
    "\n",
    "            # Use numeric label from class_to_idx\n",
    "            label = class_to_idx.get(class_name, -1)\n",
    "            if label == -1:\n",
    "                print(f\"Warning: Class {class_name} in {split} not found in train classes!\")\n",
    "                continue\n",
    "\n",
    "            for file in os.listdir(class_dir):\n",
    "                if os.path.splitext(file)[1].lower() in valid_extensions:\n",
    "                    image_path = os.path.join(class_dir, file)\n",
    "                    data[split].append({\n",
    "                        \"file_path\": image_path,\n",
    "                        \"label\": label,\n",
    "                        \"class_name\": class_name\n",
    "                    })\n",
    "\n",
    "    train_df = pd.DataFrame(data[\"train\"])\n",
    "    val_df = pd.DataFrame(data[\"val\"])\n",
    "    test_df = pd.DataFrame(data[\"test\"])\n",
    "\n",
    "    for df, split in zip([train_df, val_df, test_df], splits):\n",
    "        if df.empty:\n",
    "            print(f\"Warning: No images found in {split} split!\")\n",
    "    \n",
    "    return train_df, val_df, test_df, class_to_idx\n",
    "\n",
    "# Step 2: Analyze and Visualize\n",
    "def analyze_and_plot_dataframes(train_df, val_df, test_df, class_to_idx, save_dir):\n",
    "    \"\"\"\n",
    "    Analyze and visualize each DataFrame.\n",
    "\n",
    "    Args:\n",
    "        train_df, val_df, test_df (pd.DataFrame): DataFrames for each split.\n",
    "        class_to_idx (dict): Mapping of class names to indices.\n",
    "        save_dir (str): Directory to save plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Basic statistics and duplicates\n",
    "    for df, split in zip([train_df, val_df, test_df], [\"Train\", \"Validation\", \"Test\"]):\n",
    "        print(f\"\\n{split} Dataset Statistics:\")\n",
    "        print(f\"Total images: {len(df)}\")\n",
    "        print(f\"Class distribution:\\n{df['class_name'].value_counts()}\")\n",
    "        print(f\"Duplicate images: {df.duplicated().sum()}\")\n",
    "\n",
    "    # Dynamic color palette for classes\n",
    "    class_names = sorted(class_to_idx.keys())\n",
    "    colors = sns.color_palette(\"husl\", len(class_names))\n",
    "    color_map = {idx: colors[i] for i, idx in enumerate(class_to_idx.values())}\n",
    "\n",
    "    # Plot class distribution for each split\n",
    "    for df, split in zip([train_df, val_df, test_df], [\"Train\", \"Validation\", \"Test\"]):\n",
    "        if df.empty:\n",
    "            continue\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        counts = df['label'].value_counts().sort_index()\n",
    "        bars = plt.bar(counts.index, counts.values, color=[color_map[i] for i in counts.index])\n",
    "        \n",
    "        plt.ylim(0, counts.max() + counts.max() * 0.1)\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.annotate(f'{int(height)}',\n",
    "                         xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                         xytext=(0, 5),\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', va='bottom')\n",
    "        \n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Class Distribution in {split} Set')\n",
    "        plt.xticks(counts.index, [class_names[i] for i in counts.index], rotation=45)\n",
    "        plt.grid(axis='y', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{split.lower()}_class_distribution.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # Plot random samples (up to 10 per class) for each split\n",
    "    for df, split in zip([train_df, val_df, test_df], [\"Train\", \"Validation\", \"Test\"]):\n",
    "        if df.empty:\n",
    "            continue\n",
    "        classes = sorted(df['class_name'].unique())\n",
    "        samples_per_class = min(10, df['class_name'].value_counts().min())\n",
    "        \n",
    "        # Calculate grid size\n",
    "        n_cols = min(5, samples_per_class)\n",
    "        n_rows = (samples_per_class + n_cols - 1) // n_cols * len(classes)\n",
    "        \n",
    "        plt.figure(figsize=(n_cols * 4, n_rows * 4))\n",
    "        plot_idx = 1\n",
    "        \n",
    "        for class_name in classes:\n",
    "            class_images = df[df['class_name'] == class_name]['file_path'].values\n",
    "            if len(class_images) == 0:\n",
    "                continue\n",
    "            samples = np.random.choice(class_images, min(samples_per_class, len(class_images)), replace=False)\n",
    "            \n",
    "            for img_path in samples:\n",
    "                plt.subplot(n_rows, n_cols, plot_idx)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                plt.imshow(img)\n",
    "                plt.title(class_name, fontsize=12)\n",
    "                plt.axis('off')\n",
    "                plot_idx += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f\"{split.lower()}_random_samples.png\"))\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3e58d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train DataFrame Preview:\n",
      "                                           file_path  label class_name\n",
      "0  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "1  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "2  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "3  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "4  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "Total train images: 3655\n",
      "\n",
      "Validation DataFrame Preview:\n",
      "                                           file_path  label class_name\n",
      "0  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "1  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "2  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "3  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "4  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "Total validation images: 311\n",
      "\n",
      "Test DataFrame Preview:\n",
      "                                           file_path  label class_name\n",
      "0  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "1  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "2  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "3  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "4  O:\\O drive\\AI\\my project\\medical image project...      0        0_N\n",
      "Total test images: 570\n",
      "\n",
      "Class to Index Mapping:\n",
      "{'0_N': 0, '1_PB': 1, '2_UDH': 2, '3_FEA': 3, '4_ADH': 4, '5_DCIS': 5, '6_IC': 6}\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Create DataFrames\n",
    "data_dir = r\"O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\Dataset\\archive\"\n",
    "train_df, val_df, test_df, class_to_idx = create_dataframes(data_dir)\n",
    "\n",
    "print(\"\\nTrain DataFrame Preview:\")\n",
    "print(train_df.head())\n",
    "print(f\"Total train images: {len(train_df)}\")\n",
    "\n",
    "print(\"\\nValidation DataFrame Preview:\")\n",
    "print(val_df.head())\n",
    "print(f\"Total validation images: {len(val_df)}\")\n",
    "\n",
    "print(\"\\nTest DataFrame Preview:\")\n",
    "print(test_df.head())\n",
    "print(f\"Total test images: {len(test_df)}\")\n",
    "\n",
    "print(\"\\nClass to Index Mapping:\")\n",
    "print(class_to_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c4e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset Statistics:\n",
      "Total images: 3655\n",
      "Class distribution:\n",
      "class_name\n",
      "1_PB      714\n",
      "5_DCIS    665\n",
      "3_FEA     624\n",
      "6_IC      519\n",
      "2_UDH     389\n",
      "4_ADH     387\n",
      "0_N       357\n",
      "Name: count, dtype: int64\n",
      "Duplicate images: 0\n",
      "\n",
      "Validation Dataset Statistics:\n",
      "Total images: 311\n",
      "Class distribution:\n",
      "class_name\n",
      "3_FEA     49\n",
      "6_IC      47\n",
      "0_N       46\n",
      "2_UDH     46\n",
      "1_PB      43\n",
      "4_ADH     41\n",
      "5_DCIS    39\n",
      "Name: count, dtype: int64\n",
      "Duplicate images: 0\n",
      "\n",
      "Test Dataset Statistics:\n",
      "Total images: 570\n",
      "Class distribution:\n",
      "class_name\n",
      "5_DCIS    85\n",
      "3_FEA     83\n",
      "2_UDH     82\n",
      "0_N       81\n",
      "6_IC      81\n",
      "1_PB      79\n",
      "4_ADH     79\n",
      "Name: count, dtype: int64\n",
      "Duplicate images: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Analyze and Visualize\n",
    "analyze_and_plot_dataframes(train_df, val_df, test_df, class_to_idx, \"data_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa609658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3655, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f08da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(311, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3ee4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(570, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1af5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Pie Chart 1: Dataset Split Distribution\n",
    "def plot_dataset_split_pie(train_df, val_df, test_df, save_dir=\"data_analysis\"):\n",
    "    \"\"\"\n",
    "    Plot a pie chart showing the distribution of samples across train, val, and test splits.\n",
    "\n",
    "    Args:\n",
    "        train_df, val_df, test_df (pd.DataFrame): DataFrames for each split.\n",
    "        save_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    sizes = [len(train_df), len(val_df), len(test_df)]\n",
    "    labels = ['Training', 'Validation', 'Test']\n",
    "    total = sum(sizes)\n",
    "    colors = sns.color_palette(\"pastel\")[:3]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        labels=labels,\n",
    "        autopct=lambda pct: f\"{pct:.1f}%\\n({int(pct/100.*total)})\",\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "        shadow=True,\n",
    "        wedgeprops={'edgecolor': 'white', 'linewidth': 1}\n",
    "    )\n",
    "\n",
    "    for text in texts:\n",
    "        text.set_fontsize(13)\n",
    "        text.set_fontweight('bold')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(11)\n",
    "\n",
    "    ax.set_title(\"Dataset Split Distribution\", fontsize=16, fontweight='bold')\n",
    "    plt.text(0, -1.3, f\"Total Samples: {total}\", fontsize=11, ha='center', style='italic')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, \"dataset_split_pie.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Pie Chart 2: Test Set Class Distribution\n",
    "def plot_test_class_pie(test_df, class_to_idx, save_dir=\"data_analysis\"):\n",
    "    \"\"\"\n",
    "    Plot a pie chart showing the class distribution in the test set.\n",
    "\n",
    "    Args:\n",
    "        test_df (pd.DataFrame): Test DataFrame with 'label' and 'class_name' columns.\n",
    "        class_to_idx (dict): Mapping of class names to numeric indices.\n",
    "        save_dir (str): Directory to save the plot.\n",
    "    \"\"\"\n",
    "    # Count samples per class\n",
    "    class_counts = test_df['label'].value_counts().sort_index()\n",
    "    sizes = class_counts.values\n",
    "    labels = [list(class_to_idx.keys())[list(class_to_idx.values()).index(i)] for i in class_counts.index]\n",
    "    total = sum(sizes)\n",
    "    colors = sns.color_palette(\"pastel\", len(labels))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 7))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        labels=labels,\n",
    "        autopct=lambda pct: f\"{pct:.1f}%\\n({int(pct/100.*total)})\",\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "        shadow=True,\n",
    "        wedgeprops={'edgecolor': 'white', 'linewidth': 1}\n",
    "    )\n",
    "\n",
    "    for text in texts:\n",
    "        text.set_fontsize(13)\n",
    "        text.set_fontweight('bold')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(11)\n",
    "\n",
    "    ax.set_title(\"Test Set Class Distribution\", fontsize=16, fontweight='bold')\n",
    "    plt.text(0, -1.3, f\"Total Test Samples: {total}\", fontsize=11, ha='center', style='italic')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, \"test_class_pie.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b20d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_df and class_to_idx are available from previous step\n",
    "# Resample train_df to balance classes\n",
    "def resample_train_df(train_df, class_to_idx, save_dir=\"data_analysis\"):\n",
    "    \"\"\"\n",
    "    Resample the training DataFrame to balance classes by oversampling minority classes.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame with 'file_path', 'label', 'class_name' columns.\n",
    "        class_to_idx (dict): Mapping of class names to numeric indices.\n",
    "        save_dir (str): Directory to save the distribution plot.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Balanced training DataFrame.\n",
    "    \"\"\"\n",
    "    # Get class counts\n",
    "    class_counts = train_df['label'].value_counts()\n",
    "    majority_count = class_counts.max()\n",
    "    majority_label = class_counts.idxmax()\n",
    "\n",
    "    # Separate DataFrames for each class\n",
    "    dfs_by_class = [train_df[train_df['label'] == label] for label in class_counts.index]\n",
    "\n",
    "    # Oversample minority classes to match majority\n",
    "    balanced_dfs = []\n",
    "    for df_class, label in zip(dfs_by_class, class_counts.index):\n",
    "        if label == majority_label:\n",
    "            balanced_dfs.append(df_class)\n",
    "        else:\n",
    "            df_oversampled = resample(\n",
    "                df_class,\n",
    "                replace=True,\n",
    "                n_samples=majority_count,\n",
    "                random_state=42\n",
    "            )\n",
    "            balanced_dfs.append(df_oversampled)\n",
    "\n",
    "    # Combine all classes\n",
    "    train_df_balanced = pd.concat(balanced_dfs)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Print new class distribution\n",
    "    print(\"New class distribution after oversampling:\")\n",
    "    print(train_df_balanced['label'].value_counts())\n",
    "\n",
    "    # Visualize new class distribution\n",
    "    class_names = sorted(class_to_idx, key=class_to_idx.get)  # Sort by index\n",
    "    colors = sns.color_palette(\"husl\", len(class_names))\n",
    "    color_map = {i: colors[i] for i in range(len(class_names))}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts = train_df_balanced['label'].value_counts().sort_index()\n",
    "    bars = plt.bar(counts.index, counts.values, color=[color_map[i] for i in counts.index])\n",
    "\n",
    "    plt.ylim(0, counts.max() + counts.max() * 0.1)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{int(height)}',\n",
    "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 5),\n",
    "                     textcoords='offset points',\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Class Distribution in Balanced Train Set')\n",
    "    plt.xticks(counts.index, [class_names[i] for i in counts.index], rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, \"train_balanced_class_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    return train_df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9073d6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution after oversampling:\n",
      "label\n",
      "1    714\n",
      "6    714\n",
      "0    714\n",
      "5    714\n",
      "4    714\n",
      "2    714\n",
      "3    714\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# After creating DataFrames and resampling\n",
    "train_df, val_df, test_df, class_to_idx = create_dataframes(data_dir)\n",
    "train_df_balanced = resample_train_df(train_df, class_to_idx)\n",
    "\n",
    "# Plot pie charts\n",
    "plot_dataset_split_pie(train_df_balanced, val_df, test_df)\n",
    "plot_test_class_pie(test_df, class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104f6ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 12:24:29,534 - INFO - Using device: cuda\n",
      "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\albumentations\\augmentations\\dropout\\cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n",
      "2025-05-29 12:24:52,227 - INFO - Filtered out 0 invalid rows\n",
      "2025-05-29 12:24:52,236 - INFO - Dataset size after filtering: 4998\n",
      "2025-05-29 12:24:52,236 - INFO - Classes: ['0_N', '1_PB', '2_UDH', '3_FEA', '4_ADH', '5_DCIS', '6_IC']\n",
      "2025-05-29 12:24:52,236 - INFO - Class to index: {'0_N': 0, '1_PB': 1, '2_UDH': 2, '3_FEA': 3, '4_ADH': 4, '5_DCIS': 5, '6_IC': 6}\n",
      "2025-05-29 12:24:52,236 - INFO - Class distribution:\n",
      "class_name\n",
      "1_PB      714\n",
      "6_IC      714\n",
      "0_N       714\n",
      "5_DCIS    714\n",
      "4_ADH     714\n",
      "2_UDH     714\n",
      "3_FEA     714\n",
      "Name: count, dtype: int64\n",
      "2025-05-29 12:24:53,656 - INFO - Filtered out 0 invalid rows\n",
      "2025-05-29 12:24:53,656 - INFO - Dataset size after filtering: 311\n",
      "2025-05-29 12:24:53,656 - INFO - Classes: ['0_N', '1_PB', '2_UDH', '3_FEA', '4_ADH', '5_DCIS', '6_IC']\n",
      "2025-05-29 12:24:53,656 - INFO - Class to index: {'0_N': 0, '1_PB': 1, '2_UDH': 2, '3_FEA': 3, '4_ADH': 4, '5_DCIS': 5, '6_IC': 6}\n",
      "2025-05-29 12:24:53,664 - INFO - Class distribution:\n",
      "class_name\n",
      "3_FEA     49\n",
      "6_IC      47\n",
      "0_N       46\n",
      "2_UDH     46\n",
      "1_PB      43\n",
      "4_ADH     41\n",
      "5_DCIS    39\n",
      "Name: count, dtype: int64\n",
      "2025-05-29 12:24:56,244 - INFO - Filtered out 0 invalid rows\n",
      "2025-05-29 12:24:56,244 - INFO - Dataset size after filtering: 570\n",
      "2025-05-29 12:24:56,244 - INFO - Classes: ['0_N', '1_PB', '2_UDH', '3_FEA', '4_ADH', '5_DCIS', '6_IC']\n",
      "2025-05-29 12:24:56,244 - INFO - Class to index: {'0_N': 0, '1_PB': 1, '2_UDH': 2, '3_FEA': 3, '4_ADH': 4, '5_DCIS': 5, '6_IC': 6}\n",
      "2025-05-29 12:24:56,252 - INFO - Class distribution:\n",
      "class_name\n",
      "5_DCIS    85\n",
      "3_FEA     83\n",
      "2_UDH     82\n",
      "0_N       81\n",
      "6_IC      81\n",
      "1_PB      79\n",
      "4_ADH     79\n",
      "Name: count, dtype: int64\n",
      "2025-05-29 12:24:56,254 - INFO - Number of classes: 7\n",
      "2025-05-29 12:24:56,255 - INFO - Classes: ['0_N', '1_PB', '2_UDH', '3_FEA', '4_ADH', '5_DCIS', '6_IC']\n",
      "2025-05-29 12:24:56,255 - INFO - Training dataset size: 4998\n",
      "2025-05-29 12:24:56,256 - INFO - Validation dataset size: 311\n",
      "2025-05-29 12:24:56,257 - INFO - Test dataset size: 570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "536206"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Handle truncated/corrupted images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Define preprocessing transforms with albumentations\n",
    "def get_transforms(split, mean=(0.7531985640525818, 0.599432110786438, 0.7416298985481262), \n",
    "                  std=(0.21170948445796967, 0.2636403441429138, 0.19742192327976227)):\n",
    "    \"\"\"Define Albumentations transforms for train/val/test splits.\"\"\"\n",
    "    if split == \"train\":\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(limit=90, p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n",
    "            A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, p=0.5),\n",
    "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.RandomResizedCrop(224, 224, scale=(0.8, 1.0), p=0.5),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224, always_apply=True),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# Wrapper to use albumentations with PyTorch\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        augmented = self.transform(image=img)\n",
    "        return augmented['image']\n",
    "\n",
    "# Custom Dataset for DataFrame\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(self.df['class_name'].unique())\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.df = self._validate_and_filter_dataset()\n",
    "        unique_labels = sorted(self.df['label'].unique())\n",
    "        expected_labels = list(range(len(self.classes)))\n",
    "        if sorted(unique_labels) != expected_labels:\n",
    "            logger.error(f\"Label mismatch: Expected labels {expected_labels}, found {unique_labels}\")\n",
    "            raise ValueError(f\"Label mismatch: Expected {expected_labels}, found {unique_labels}\")\n",
    "        logger.info(f\"Dataset size after filtering: {len(self.df)}\")\n",
    "        logger.info(f\"Classes: {self.classes}\")\n",
    "        logger.info(f\"Class to index: {self.class_to_idx}\")\n",
    "        logger.info(f\"Class distribution:\\n{self.df['class_name'].value_counts()}\")\n",
    "\n",
    "    def _validate_and_filter_dataset(self):\n",
    "        valid_rows = []\n",
    "        for idx in range(len(self.df)):\n",
    "            row = self.df.iloc[idx]\n",
    "            img_path = row['file_path']\n",
    "            label = row['label']\n",
    "            if not os.path.isfile(img_path):\n",
    "                logger.warning(f\"Invalid file path at index {idx}: {img_path}\")\n",
    "                continue\n",
    "            if not isinstance(label, (int, np.integer)) or label not in range(len(self.classes)):\n",
    "                logger.warning(f\"Invalid label at index {idx}: {label}\")\n",
    "                continue\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img.close()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Corrupted image at index {idx}: {img_path}, error: {e}\")\n",
    "                continue\n",
    "            valid_rows.append(idx)\n",
    "        if not valid_rows:\n",
    "            raise ValueError(\"No valid items in dataset after filtering\")\n",
    "        filtered_df = self.df.iloc[valid_rows].reset_index(drop=True)\n",
    "        logger.info(f\"Filtered out {len(self.df) - len(filtered_df)} invalid rows\")\n",
    "        return filtered_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row['file_path']\n",
    "        label = row['label']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        if label_tensor.numel() != 1:\n",
    "            logger.error(f\"Label at index {idx} is not scalar: {label_tensor}\")\n",
    "            return None\n",
    "        logger.debug(f\"Item {idx}: label {label} -> tensor {label_tensor}\")\n",
    "        return img, label_tensor\n",
    "\n",
    "# Custom Dataset Wrapper\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.dataset, name)\n",
    "\n",
    "# # CutMix and MixUp Functions\n",
    "# def rand_bbox(size, lam):\n",
    "#     W = size[2]\n",
    "#     H = size[3]\n",
    "#     cut_rat = np.sqrt(1. - lam)\n",
    "#     cut_w = int(W * cut_rat)\n",
    "#     cut_h = int(H * cut_rat)\n",
    "#     cx = np.random.randint(W)\n",
    "#     cy = np.random.randint(H)\n",
    "#     bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "#     bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "#     bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "#     bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "#     return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# def cutmix(data, targets, alpha=1.0):\n",
    "#     if not isinstance(targets, torch.Tensor):\n",
    "#         raise ValueError(f\"CutMix expects tensor targets, got {type(targets)}\")\n",
    "#     batch_size = data.size(0)\n",
    "#     index = torch.randperm(batch_size)\n",
    "#     shuffled_data = data[index]\n",
    "#     shuffled_targets = targets[index]\n",
    "#     lam = np.random.beta(alpha, alpha)\n",
    "#     bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "#     data[:, :, bbx1:bbx2, bby1:bby2] = shuffled_data[:, :, bbx1:bbx2, bby1:bby2]\n",
    "#     lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size(-1) * data.size(-2)))\n",
    "#     return data, (targets, shuffled_targets, lam)\n",
    "\n",
    "# def mixup(data, targets, alpha=0.2):\n",
    "#     if not isinstance(targets, torch.Tensor):\n",
    "#         raise ValueError(f\"MixUp expects tensor targets, got {type(targets)}\")\n",
    "#     batch_size = data.size(0)\n",
    "#     index = torch.randperm(batch_size)\n",
    "#     shuffled_data = data[index]\n",
    "#     shuffled_targets = targets[index]\n",
    "#     lam = np.random.beta(alpha, alpha)\n",
    "#     mixed_data = lam * data + (1 - lam) * shuffled_data\n",
    "#     return mixed_data, (targets, shuffled_targets, lam)\n",
    "\n",
    "# Custom Collate Function\n",
    "# def custom_collate(batch):\n",
    "#     batch = [item for item in batch if item is not None]\n",
    "#     if len(batch) == 0:\n",
    "#         logger.warning(\"Empty batch after filtering None items\")\n",
    "#         return None, None\n",
    "#     try:\n",
    "#         data = torch.stack([item[0] for item in batch])\n",
    "#         logger.debug(f\"Data stacked: shape {data.shape}, type {type(data)}\")\n",
    "#         targets = []\n",
    "#         for idx, item in enumerate(batch):\n",
    "#             label = item[1]\n",
    "#             if not isinstance(label, torch.Tensor):\n",
    "#                 logger.error(f\"Non-tensor label at batch index {idx}: {type(label)}\")\n",
    "#                 return None, None\n",
    "#             if label.numel() != 1:\n",
    "#                 logger.error(f\"Label at batch index {idx} is not scalar: {label}\")\n",
    "#                 return None, None\n",
    "#             targets.append(label.item())\n",
    "#         targets = torch.tensor(targets, dtype=torch.long)\n",
    "#         logger.debug(f\"Targets before augmentation: {targets}, type {type(targets)}\")\n",
    "#         augmentation = random.choices(['none', 'mixup', 'cutmix'], weights=[0.6, 0.2, 0.2], k=1)[0]\n",
    "#         logger.debug(f\"Applying augmentation: {augmentation}\")\n",
    "#         if augmentation == 'mixup':\n",
    "#             data, targets = mixup(data, targets, alpha=0.2)\n",
    "#             if not isinstance(targets, tuple) or len(targets) != 3:\n",
    "#                 logger.error(f\"MixUp returned invalid targets: {type(targets)}\")\n",
    "#                 return None, None\n",
    "#             labels1, labels2, lam = targets\n",
    "#             if not (isinstance(labels1, torch.Tensor) and isinstance(labels2, torch.Tensor) and isinstance(lam, float)):\n",
    "#                 logger.error(f\"MixUp targets tuple invalid: {type(labels1)}, {type(labels2)}, {type(lam)}\")\n",
    "#                 return None, None\n",
    "#             logger.debug(f\"After MixUp: data shape {data.shape}, targets {type(targets)} (labels1: {labels1.shape}, labels2: {labels2.shape}, lam: {lam})\")\n",
    "#         elif augmentation == 'cutmix':\n",
    "#             data, targets = cutmix(data, targets, alpha=1.0)\n",
    "#             if not isinstance(targets, tuple) or len(targets) != 3:\n",
    "#                 logger.error(f\"CutMix returned invalid targets: {type(targets)}\")\n",
    "#                 return None, None\n",
    "#             labels1, labels2, lam = targets\n",
    "#             if not (isinstance(labels1, torch.Tensor) and isinstance(labels2, torch.Tensor) and isinstance(lam, float)):\n",
    "#                 logger.error(f\"CutMix targets tuple invalid: {type(labels1)}, {type(labels2)}, {type(lam)}\")\n",
    "#                 return None, None\n",
    "#             logger.debug(f\"After CutMix: data shape {data.shape}, targets {type(targets)} (labels1: {labels1.shape}, labels2: {labels2.shape}, lam: {lam})\")\n",
    "#         else:\n",
    "#             logger.debug(f\"No augmentation, targets remain tensor: {targets.shape}\")\n",
    "#         return data, targets\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error in custom_collate: {e}\")\n",
    "#         return None, None\n",
    "\n",
    "\n",
    "# Create Datasets (Assuming train_df_balanced, val_df, test_df are defined)\n",
    "try:\n",
    "    train_dataset = DataFrameDataset(train_df_balanced, transform=AlbumentationsTransform(get_transforms(\"train\")))\n",
    "    val_dataset = DataFrameDataset(val_df, transform=AlbumentationsTransform(get_transforms(\"val\")))\n",
    "    test_dataset = DataFrameDataset(test_df, transform=AlbumentationsTransform(get_transforms(\"test\")))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating datasets: {e}\")\n",
    "    raise\n",
    "\n",
    "# Wrap Datasets\n",
    "train_dataset_custom = CustomDataset(train_dataset)\n",
    "val_dataset_custom = CustomDataset(val_dataset)\n",
    "test_dataset_custom = CustomDataset(test_dataset)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Create Data Loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_custom,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    "    #collate_fn=custom_collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_custom,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_custom,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Dataset Summary\n",
    "logger.info(f\"Number of classes: {num_classes}\")\n",
    "logger.info(f\"Classes: {train_dataset.classes}\")\n",
    "logger.info(f\"Training dataset size: {len(train_loader.dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_loader.dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Memory Optimization\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c8f12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 12:25:17,146 - INFO - Using device: cuda\n",
      "2025-05-29 12:25:19,344 - INFO - ✅ Batch 1 Loaded in 0.0256 sec\n",
      "2025-05-29 12:25:19,351 - INFO - Batch 1 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:19,351 - INFO - Batch 1 labels: tensor([1, 0, 4, 0, 3, 6, 4, 6, 2, 6, 4, 1, 5, 4, 4, 5, 3, 1, 3, 1, 4, 6, 1, 4,\n",
      "        3, 4, 6, 6, 3, 0, 0, 5, 0, 0, 6, 2, 1, 0, 3, 3, 5, 5, 1, 3, 5, 6, 4, 5,\n",
      "        4, 2, 5, 5, 3, 3, 4, 6, 2, 6, 3, 5, 1, 0, 2, 5], device='cuda:0')\n",
      "2025-05-29 12:25:21,337 - INFO - ✅ Batch 2 Loaded in 0.0246 sec\n",
      "2025-05-29 12:25:21,337 - INFO - Batch 2 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:21,337 - INFO - Batch 2 labels: tensor([0, 1, 1, 3, 0, 5, 6, 6, 0, 5, 1, 0, 3, 6, 0, 4, 6, 4, 1, 4, 4, 2, 0, 1,\n",
      "        4, 4, 0, 5, 0, 5, 0, 5, 2, 6, 0, 1, 5, 1, 1, 2, 6, 1, 1, 0, 3, 6, 2, 1,\n",
      "        0, 1, 2, 1, 5, 2, 2, 6, 2, 0, 6, 6, 6, 1, 4, 4], device='cuda:0')\n",
      "2025-05-29 12:25:23,554 - INFO - ✅ Batch 3 Loaded in 0.0080 sec\n",
      "2025-05-29 12:25:23,554 - INFO - Batch 3 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:23,562 - INFO - Batch 3 labels: tensor([3, 0, 6, 3, 0, 1, 0, 1, 4, 6, 4, 2, 5, 4, 1, 5, 0, 0, 3, 1, 2, 3, 1, 1,\n",
      "        4, 5, 3, 0, 6, 2, 4, 5, 3, 4, 6, 1, 4, 1, 5, 3, 1, 5, 1, 6, 5, 6, 2, 4,\n",
      "        2, 5, 4, 2, 3, 3, 6, 5, 5, 2, 1, 3, 0, 2, 5, 1], device='cuda:0')\n",
      "2025-05-29 12:25:25,913 - INFO - ✅ Batch 4 Loaded in 0.0075 sec\n",
      "2025-05-29 12:25:25,913 - INFO - Batch 4 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:25,913 - INFO - Batch 4 labels: tensor([3, 3, 5, 6, 6, 4, 3, 0, 0, 0, 2, 0, 0, 3, 6, 1, 3, 5, 5, 3, 1, 6, 5, 0,\n",
      "        2, 4, 3, 4, 3, 6, 0, 1, 1, 3, 5, 4, 5, 0, 3, 2, 5, 4, 5, 0, 3, 1, 2, 4,\n",
      "        3, 5, 2, 5, 4, 2, 3, 0, 0, 5, 5, 0, 2, 0, 0, 3], device='cuda:0')\n",
      "2025-05-29 12:25:28,156 - INFO - ✅ Batch 5 Loaded in 0.0237 sec\n",
      "2025-05-29 12:25:28,156 - INFO - Batch 5 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:28,156 - INFO - Batch 5 labels: tensor([2, 0, 4, 3, 2, 4, 1, 2, 1, 2, 3, 5, 5, 2, 4, 3, 2, 1, 2, 4, 2, 1, 3, 2,\n",
      "        1, 0, 6, 3, 3, 1, 1, 6, 2, 2, 2, 6, 4, 6, 6, 0, 5, 0, 0, 4, 6, 6, 1, 3,\n",
      "        6, 4, 2, 5, 1, 1, 5, 1, 0, 0, 4, 4, 5, 4, 0, 1], device='cuda:0')\n",
      "2025-05-29 12:25:30,247 - INFO - ✅ Batch 6 Loaded in 0.0241 sec\n",
      "2025-05-29 12:25:30,247 - INFO - Batch 6 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:30,255 - INFO - Batch 6 labels: tensor([2, 1, 1, 6, 4, 1, 4, 3, 3, 2, 0, 0, 3, 1, 3, 5, 1, 1, 6, 3, 0, 6, 3, 3,\n",
      "        5, 1, 2, 3, 0, 1, 4, 2, 0, 5, 3, 6, 0, 0, 5, 4, 5, 2, 6, 2, 4, 5, 4, 6,\n",
      "        5, 6, 4, 3, 1, 4, 1, 3, 5, 6, 3, 6, 0, 1, 1, 4], device='cuda:0')\n",
      "2025-05-29 12:25:32,449 - INFO - ✅ Batch 7 Loaded in 0.0240 sec\n",
      "2025-05-29 12:25:32,449 - INFO - Batch 7 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:32,457 - INFO - Batch 7 labels: tensor([6, 4, 1, 0, 0, 6, 4, 0, 6, 0, 6, 4, 3, 2, 6, 1, 6, 4, 0, 5, 2, 6, 1, 4,\n",
      "        3, 3, 5, 3, 5, 1, 3, 5, 5, 3, 1, 4, 0, 1, 0, 4, 0, 3, 4, 0, 6, 5, 0, 1,\n",
      "        6, 2, 1, 1, 6, 4, 0, 5, 1, 5, 6, 1, 2, 4, 1, 0], device='cuda:0')\n",
      "2025-05-29 12:25:34,597 - INFO - ✅ Batch 8 Loaded in 0.0255 sec\n",
      "2025-05-29 12:25:34,597 - INFO - Batch 8 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:34,604 - INFO - Batch 8 labels: tensor([2, 4, 0, 2, 2, 4, 5, 5, 4, 0, 0, 4, 3, 1, 4, 4, 1, 3, 4, 6, 1, 2, 3, 6,\n",
      "        4, 0, 1, 2, 1, 5, 3, 3, 5, 6, 2, 4, 2, 0, 5, 5, 4, 2, 6, 1, 0, 1, 3, 0,\n",
      "        3, 3, 6, 5, 2, 6, 1, 0, 4, 4, 0, 2, 1, 6, 0, 4], device='cuda:0')\n",
      "2025-05-29 12:25:37,055 - INFO - ✅ Batch 9 Loaded in 0.0252 sec\n",
      "2025-05-29 12:25:37,055 - INFO - Batch 9 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:37,063 - INFO - Batch 9 labels: tensor([2, 1, 2, 3, 3, 6, 0, 1, 5, 0, 2, 6, 5, 0, 2, 0, 6, 4, 6, 4, 1, 2, 6, 1,\n",
      "        1, 2, 5, 1, 5, 4, 6, 3, 2, 6, 5, 1, 1, 0, 6, 5, 0, 3, 2, 2, 2, 5, 2, 2,\n",
      "        3, 5, 1, 5, 2, 0, 5, 0, 5, 5, 3, 3, 5, 4, 1, 5], device='cuda:0')\n",
      "2025-05-29 12:25:39,054 - INFO - ✅ Batch 10 Loaded in 0.0245 sec\n",
      "2025-05-29 12:25:39,054 - INFO - Batch 10 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:39,054 - INFO - Batch 10 labels: tensor([4, 1, 6, 1, 5, 4, 0, 4, 6, 4, 2, 0, 6, 5, 1, 3, 6, 1, 2, 5, 5, 1, 2, 1,\n",
      "        5, 5, 5, 2, 2, 4, 3, 1, 2, 2, 3, 3, 4, 4, 1, 0, 1, 2, 0, 6, 6, 2, 1, 1,\n",
      "        4, 5, 0, 5, 0, 5, 4, 1, 3, 5, 1, 1, 6, 5, 3, 6], device='cuda:0')\n",
      "2025-05-29 12:25:41,367 - INFO - ✅ Batch 11 Loaded in 0.0238 sec\n",
      "2025-05-29 12:25:41,367 - INFO - Batch 11 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:41,367 - INFO - Batch 11 labels: tensor([2, 2, 0, 5, 5, 5, 2, 5, 3, 1, 1, 5, 3, 0, 2, 1, 5, 0, 0, 4, 1, 5, 3, 0,\n",
      "        2, 3, 1, 6, 2, 4, 4, 3, 4, 2, 5, 5, 0, 6, 3, 4, 4, 6, 5, 4, 1, 3, 4, 1,\n",
      "        0, 5, 1, 2, 5, 4, 6, 1, 2, 0, 1, 0, 6, 4, 3, 6], device='cuda:0')\n",
      "2025-05-29 12:25:43,513 - INFO - ✅ Batch 12 Loaded in 0.0246 sec\n",
      "2025-05-29 12:25:43,513 - INFO - Batch 12 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:43,521 - INFO - Batch 12 labels: tensor([1, 1, 4, 3, 4, 2, 4, 4, 2, 0, 4, 0, 3, 1, 3, 2, 6, 6, 5, 3, 5, 1, 3, 1,\n",
      "        6, 3, 0, 1, 5, 4, 3, 2, 3, 5, 0, 1, 4, 2, 5, 3, 0, 6, 4, 5, 4, 2, 4, 2,\n",
      "        1, 3, 0, 4, 0, 5, 3, 0, 2, 3, 5, 5, 5, 0, 6, 1], device='cuda:0')\n",
      "2025-05-29 12:25:46,088 - INFO - ✅ Batch 13 Loaded in 0.0239 sec\n",
      "2025-05-29 12:25:46,096 - INFO - Batch 13 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:46,097 - INFO - Batch 13 labels: tensor([2, 0, 1, 5, 3, 2, 1, 3, 3, 0, 3, 0, 6, 2, 1, 5, 6, 2, 4, 3, 6, 2, 3, 3,\n",
      "        2, 1, 1, 6, 1, 6, 1, 1, 2, 2, 3, 0, 2, 0, 4, 0, 6, 1, 1, 5, 5, 1, 4, 0,\n",
      "        3, 2, 1, 4, 6, 2, 0, 0, 2, 2, 5, 4, 2, 4, 5, 3], device='cuda:0')\n",
      "2025-05-29 12:25:48,082 - INFO - ✅ Batch 14 Loaded in 0.0210 sec\n",
      "2025-05-29 12:25:48,082 - INFO - Batch 14 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:48,091 - INFO - Batch 14 labels: tensor([2, 6, 2, 6, 4, 1, 3, 2, 2, 6, 5, 5, 0, 3, 5, 1, 4, 1, 6, 1, 3, 6, 1, 0,\n",
      "        5, 0, 2, 2, 1, 4, 5, 1, 1, 5, 5, 0, 0, 5, 3, 1, 4, 2, 2, 1, 4, 4, 6, 4,\n",
      "        1, 0, 2, 1, 4, 0, 3, 4, 6, 3, 3, 0, 0, 4, 2, 3], device='cuda:0')\n",
      "2025-05-29 12:25:50,333 - INFO - ✅ Batch 15 Loaded in 0.0240 sec\n",
      "2025-05-29 12:25:50,333 - INFO - Batch 15 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:50,341 - INFO - Batch 15 labels: tensor([0, 1, 4, 3, 2, 3, 1, 0, 3, 0, 1, 3, 0, 1, 6, 0, 1, 1, 0, 6, 5, 0, 0, 2,\n",
      "        5, 4, 4, 3, 4, 5, 1, 6, 0, 2, 3, 4, 3, 1, 6, 5, 5, 2, 0, 1, 6, 3, 5, 2,\n",
      "        6, 2, 3, 5, 6, 1, 3, 3, 2, 1, 5, 4, 0, 6, 6, 6], device='cuda:0')\n",
      "2025-05-29 12:25:52,734 - INFO - ✅ Batch 16 Loaded in 0.0244 sec\n",
      "2025-05-29 12:25:52,734 - INFO - Batch 16 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:52,742 - INFO - Batch 16 labels: tensor([5, 3, 3, 4, 0, 0, 2, 5, 5, 2, 1, 4, 0, 2, 0, 1, 0, 4, 5, 4, 3, 5, 0, 4,\n",
      "        2, 6, 5, 0, 4, 1, 4, 6, 5, 2, 1, 0, 6, 3, 0, 0, 1, 6, 5, 4, 1, 1, 0, 3,\n",
      "        5, 0, 3, 4, 3, 5, 6, 4, 0, 1, 2, 0, 0, 3, 3, 4], device='cuda:0')\n",
      "2025-05-29 12:25:54,789 - INFO - ✅ Batch 17 Loaded in 0.0163 sec\n",
      "2025-05-29 12:25:54,789 - INFO - Batch 17 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:54,789 - INFO - Batch 17 labels: tensor([4, 3, 4, 0, 2, 2, 6, 1, 4, 2, 0, 2, 4, 3, 2, 6, 3, 4, 2, 5, 4, 6, 3, 5,\n",
      "        6, 4, 1, 5, 4, 6, 5, 3, 0, 0, 1, 4, 6, 1, 4, 6, 6, 1, 3, 5, 1, 2, 3, 0,\n",
      "        5, 0, 4, 3, 4, 4, 1, 0, 3, 4, 4, 2, 6, 1, 5, 5], device='cuda:0')\n",
      "2025-05-29 12:25:56,907 - INFO - ✅ Batch 18 Loaded in 0.0241 sec\n",
      "2025-05-29 12:25:56,907 - INFO - Batch 18 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:56,907 - INFO - Batch 18 labels: tensor([2, 3, 5, 4, 0, 6, 5, 1, 4, 0, 1, 5, 6, 4, 0, 5, 5, 6, 6, 6, 1, 3, 1, 6,\n",
      "        1, 4, 6, 6, 5, 4, 1, 0, 6, 6, 6, 4, 2, 6, 6, 5, 0, 3, 3, 5, 5, 6, 0, 6,\n",
      "        1, 3, 4, 1, 2, 2, 5, 2, 1, 1, 1, 0, 0, 3, 3, 5], device='cuda:0')\n",
      "2025-05-29 12:25:58,946 - INFO - ✅ Batch 19 Loaded in 0.0159 sec\n",
      "2025-05-29 12:25:58,946 - INFO - Batch 19 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:25:58,946 - INFO - Batch 19 labels: tensor([3, 4, 1, 1, 5, 2, 4, 0, 0, 3, 1, 1, 3, 2, 0, 4, 6, 1, 2, 2, 6, 4, 6, 4,\n",
      "        1, 4, 3, 0, 6, 3, 3, 5, 5, 3, 3, 3, 5, 6, 2, 1, 5, 3, 3, 4, 2, 1, 2, 6,\n",
      "        1, 6, 1, 4, 4, 5, 1, 4, 0, 3, 6, 6, 0, 6, 3, 6], device='cuda:0')\n",
      "2025-05-29 12:26:01,241 - INFO - ✅ Batch 20 Loaded in 0.0235 sec\n",
      "2025-05-29 12:26:01,241 - INFO - Batch 20 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:26:01,249 - INFO - Batch 20 labels: tensor([2, 2, 0, 3, 6, 1, 6, 2, 3, 3, 1, 4, 1, 4, 2, 2, 2, 1, 4, 1, 5, 6, 3, 2,\n",
      "        2, 2, 2, 6, 6, 3, 1, 3, 1, 3, 2, 5, 2, 5, 1, 2, 3, 5, 6, 6, 4, 2, 2, 5,\n",
      "        4, 4, 0, 6, 0, 2, 5, 5, 4, 4, 3, 5, 6, 4, 2, 6], device='cuda:0')\n",
      "2025-05-29 12:26:03,335 - INFO - ✅ Batch 21 Loaded in 0.0246 sec\n",
      "2025-05-29 12:26:03,335 - INFO - Batch 21 inputs shape: torch.Size([64, 3, 224, 224])\n",
      "2025-05-29 12:26:03,343 - INFO - Batch 21 labels: tensor([3, 4, 1, 0, 3, 1, 4, 6, 4, 6, 0, 3, 2, 4, 2, 4, 0, 6, 0, 6, 3, 0, 1, 1,\n",
      "        1, 4, 4, 5, 3, 5, 0, 5, 4, 0, 3, 0, 1, 2, 5, 4, 6, 5, 1, 0, 3, 5, 6, 3,\n",
      "        2, 5, 5, 3, 1, 0, 3, 0, 5, 6, 2, 4, 0, 5, 1, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Select CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Check batch loading time\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Skip invalid batches\n",
    "    if inputs is None or labels is None:\n",
    "        logger.warning(f\"Skipping empty batch {i+1}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if not isinstance(inputs, torch.Tensor):\n",
    "            logger.error(f\"Batch {i+1} inputs is not a tensor: {type(inputs)}\")\n",
    "            continue\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Handle labels (tensor or tuple)\n",
    "        if isinstance(labels, tuple):\n",
    "            labels1, labels2, lam = labels\n",
    "            if not isinstance(labels1, torch.Tensor) or not isinstance(labels2, torch.Tensor):\n",
    "                logger.error(f\"Batch {i+1} labels tuple contains non-tensor: {type(labels1)}, {type(labels2)}\")\n",
    "                continue\n",
    "            labels = (labels1.to(device), labels2.to(device), lam)\n",
    "        else:\n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                logger.error(f\"Batch {i+1} labels is not a tensor: {type(labels)}\")\n",
    "                continue\n",
    "            labels = labels.to(device)\n",
    "\n",
    "        batch_time = time.time() - start_time\n",
    "        logger.info(f\"✅ Batch {i+1} Loaded in {batch_time:.4f} sec\")\n",
    "        logger.info(f\"Batch {i+1} inputs shape: {inputs.shape}\")\n",
    "        logger.info(f\"Batch {i+1} labels: {labels}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing batch {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if i == 20:\n",
    "        break\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0274628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eafe899c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db74c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ad57e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_class_weights(dataset, num_classes, device):\n",
    "#     \"\"\"\n",
    "#     Compute balanced class weights for a dataset.\n",
    "    \n",
    "#     Args:\n",
    "#         dataset: Dataset object containing images and labels.\n",
    "#         num_classes: Integer, number of classes (e.g., 7 for BraCS).\n",
    "#         device: torch.device, device to move weights to (e.g., 'cuda').\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Tensor of class weights on the specified device.\n",
    "#     \"\"\"\n",
    "#     import logging\n",
    "#     logger = logging.getLogger(__name__)\n",
    "    \n",
    "#     labels = []\n",
    "#     for idx, (_, label) in enumerate(dataset):\n",
    "#         if label is None:\n",
    "#             logger.warning(f\"None label at index {idx}\")\n",
    "#             continue\n",
    "#         if not isinstance(label, torch.Tensor):\n",
    "#             logger.error(f\"Non-tensor label at index {idx}: {type(label)}\")\n",
    "#             continue\n",
    "#         labels.append(label.item())\n",
    "    \n",
    "#     if not labels:\n",
    "#         raise ValueError(\"No valid labels found in dataset\")\n",
    "    \n",
    "#     unique_labels = sorted(labels)\n",
    "#     expected_classes = list(range(num_classes))\n",
    "#     if unique_labels != expected_classes:\n",
    "#         logger.error(f\"Missing labels: Expected {expected_classes}, found {unique_labels}\")\n",
    "#         raise ValueError(f\"Dataset labels {unique_labels} do not match expected classes {expected_classes}\")\n",
    "    \n",
    "#     # Log class distribution\n",
    "#     from collections import Counter\n",
    "#     label_counts = Counter(labels)\n",
    "#     logger.info(f\"Class distribution: {dict(sorted(label_counts.items()))}\")\n",
    "    \n",
    "#     from sklearn.utils.class_weight import compute_class_weight\n",
    "#     class_weights = compute_class_weight('balanced', classes=np.array(expected_classes), y=labels)\n",
    "#     logger.info(f\"Class weights: {class_weights.tolist()}\")\n",
    "    \n",
    "#     return torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63eaa545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f50fd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import os\n",
    "from torchvision import models\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights, DenseNet201_Weights, EfficientNet_B0_Weights, ResNet18_Weights, ResNet50_Weights, ResNeXt101_32X8D_Weights\n",
    "\n",
    "# Assuming these are from your previous code\n",
    "# from your_data_preparation import train_dataset, train_loader, val_loader, test_loader, DataFrameDataset\n",
    "# from your_modeling import compute_class_weights\n",
    "\n",
    "# CBAM Components\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // ratio, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_channels // ratio, in_channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out).view(b, c, 1, 1)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = self.conv(out)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttention(in_channels)\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ca = self.channel_attention(x)\n",
    "        x = x * ca\n",
    "        sa = self.spatial_attention(x)\n",
    "        x = x * sa\n",
    "        return x\n",
    "\n",
    "# Insert CBAM into Model\n",
    "def insert_cbam(model, cbam_channels):\n",
    "    cbam = CBAM(cbam_channels)\n",
    "    if isinstance(model, models.ResNet):\n",
    "        model.cbam = cbam\n",
    "        original_forward = model.forward\n",
    "        def new_forward(self, x):\n",
    "            x = self.conv1(x)\n",
    "            x = self.cbam(x)\n",
    "            x = self.bn1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.maxpool(x)\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.layer4(x)\n",
    "            x = self.avgpool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "        model.forward = new_forward.__get__(model)\n",
    "    elif isinstance(model, (models.DenseNet, models.EfficientNet)):\n",
    "        features = model.features\n",
    "        new_features = nn.Sequential(\n",
    "            features[0],\n",
    "            cbam,\n",
    "            *features[1:]\n",
    "        )\n",
    "        model.features = new_features\n",
    "    else:\n",
    "        raise ValueError(\"Model type not supported for CBAM insertion.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, verbose=False, delta=0, save_dir=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def __call__(self, val_loss, epoch, model_weights, model_name_prefix):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "            self.save_best_weights(model_weights, model_name_prefix)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(f\"Early stopping triggered after {self.counter} epochs of no improvement.\")\n",
    "\n",
    "    def save_best_weights(self, model_weights, model_name_prefix):\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        model_name = os.path.join(self.save_dir, f\"{model_name_prefix}_epoch_{self.best_epoch + 1}.pth\")\n",
    "        torch.save(model_weights, model_name)\n",
    "        if self.verbose:\n",
    "            print(f\"✅ Best model weights saved to {model_name}.\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights, DenseNet201_Weights, EfficientNet_B0_Weights, ResNet18_Weights, ResNet50_Weights, ResNeXt101_32X8D_Weights\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights, DenseNet201_Weights, EfficientNet_B0_Weights, ResNet18_Weights, ResNet50_Weights, ResNeXt101_32X8D_Weights\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Placeholder for insert_cbam\n",
    "def insert_cbam(model, cbam_channels):\n",
    "    \"\"\"\n",
    "    Insert CBAM modules into the model.\n",
    "    Args:\n",
    "        model: PyTorch model (e.g., ResNeXt, EfficientNet).\n",
    "        cbam_channels: Number of channels for CBAM.\n",
    "    Returns:\n",
    "        Modified model with CBAM.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Inserting CBAM with {cbam_channels} channels\")\n",
    "    return model  # Replace with actual CBAM insertion\n",
    "\n",
    "# CustomClassifier (unchanged)\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Placeholder for SupConLoss\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, features, labels):\n",
    "        # Replace with actual implementation\n",
    "        return torch.tensor(0.0, device=features.device)\n",
    "\n",
    "# Updated ContrastiveModel\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, base_model, num_classes, feature_dim):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Store backbone\n",
    "        self.backbone = base_model\n",
    "        \n",
    "        # Determine classifier attribute\n",
    "        if hasattr(self.backbone, 'fc'):\n",
    "            self.classifier_attr = 'fc'\n",
    "        elif hasattr(self.backbone, 'classifier'):\n",
    "            self.classifier_attr = 'classifier'\n",
    "        else:\n",
    "            raise ValueError(\"Backbone has neither 'fc' nor 'classifier' attribute\")\n",
    "        \n",
    "        # Create feature extractor by removing the final classifier\n",
    "        self.feature_extractor = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "        \n",
    "        # Projection head uses feature_dim (e.g., 2048 for resnext50)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features before the final classifier\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten: [batch_size, feature_dim]\n",
    "        \n",
    "        # Get logits from the classifier\n",
    "        classifier = getattr(self.backbone, self.classifier_attr)\n",
    "        logits = classifier(features)  # [batch_size, num_classes]\n",
    "        \n",
    "        # Get projections\n",
    "        proj = self.projection_head(features)  # [batch_size, 128]\n",
    "        \n",
    "        return logits, proj\n",
    "\n",
    "# Updated get_model Function\n",
    "def get_model(model_name, num_classes, device):\n",
    "    \"\"\"\n",
    "    Load and configure a model with CBAM and contrastive learning.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model ('resnext50', 'efficientnet_b0', etc.).\n",
    "        num_classes: Number of output classes (7 for BraCS).\n",
    "        device: Device to run the model on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "        Configured model.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading model: {model_name} with {num_classes} classes on {device}\")\n",
    "\n",
    "    # Load base model with pretrained weights\n",
    "    if model_name == \"resnext50\":\n",
    "        base_model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.DEFAULT)\n",
    "        cbam_channels = 64\n",
    "    elif model_name == \"densenet201\":\n",
    "        base_model = models.densenet201(weights=DenseNet201_Weights.DEFAULT)\n",
    "        cbam_channels = 64\n",
    "    elif model_name == \"efficientnet_b0\":\n",
    "        base_model = models.efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        cbam_channels = 32\n",
    "    elif model_name == \"resnet18\":\n",
    "        base_model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        cbam_channels = 64\n",
    "    elif model_name == \"resnet50\":\n",
    "        base_model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        cbam_channels = 64\n",
    "    elif model_name == \"resnext101\":\n",
    "        base_model = models.resnext101_32x8d(weights=ResNeXt101_32X8D_Weights.DEFAULT)\n",
    "        cbam_channels = 64\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_name} not supported.\")\n",
    "\n",
    "    # Insert CBAM\n",
    "    base_model = insert_cbam(base_model, cbam_channels)\n",
    "\n",
    "    # Determine input features for classifier\n",
    "    if hasattr(base_model, 'fc'):\n",
    "        in_features = base_model.fc.in_features\n",
    "        classifier_attr = 'fc'\n",
    "    elif hasattr(base_model, 'classifier'):\n",
    "        if isinstance(base_model.classifier, nn.Linear):\n",
    "            in_features = base_model.classifier.in_features\n",
    "        elif isinstance(base_model.classifier, nn.Sequential):\n",
    "            for layer in reversed(base_model.classifier):\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    in_features = layer.in_features\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"No Linear layer found in classifier\")\n",
    "        classifier_attr = 'classifier'\n",
    "    else:\n",
    "        raise ValueError(\"Cannot find classifier layer (neither 'fc' nor 'classifier')\")\n",
    "\n",
    "    # Replace classifier with CustomClassifier\n",
    "    custom_classifier = CustomClassifier(in_features, num_classes)\n",
    "    setattr(base_model, classifier_attr, custom_classifier)\n",
    "\n",
    "    # Initialize ContrastiveModel with feature_dim\n",
    "    try:\n",
    "        model = ContrastiveModel(base_model, num_classes, feature_dim=in_features)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize ContrastiveModel: {e}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(f\"Model {model_name} loaded successfully with {in_features} input features to classifier\")\n",
    "    return model.to(device)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import logging\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import logging\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, optimizer, scheduler, \n",
    "                     model_name_prefix, epochs=25, device=None, early_stopping=None, \n",
    "                     save_dir=r\"O:\\O drive\\AI\\my project\\medical image projects\\BraCs\", accum_steps=6):\n",
    "    \"\"\"\n",
    "    Train and validate the model with manually defined class weights, CutMix/MixUp, and fixed autocast.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model (ResNeXt50 with CBAM and contrastive learning).\n",
    "        train_loader: DataLoader for training data.\n",
    "        val_loader: DataLoader for validation data.\n",
    "        optimizer: PyTorch optimizer.\n",
    "        scheduler: Learning rate scheduler.\n",
    "        model_name_prefix: Prefix for saved model files.\n",
    "        epochs: Number of training epochs (default: 25).\n",
    "        device: Device to run the model on (e.g., torch.device('cuda')).\n",
    "        early_stopping: Optional early stopping object.\n",
    "        save_dir: Directory to save model weights and metrics.\n",
    "        accum_steps: Number of batches to accumulate gradients (default: 4).\n",
    "    \n",
    "    Returns:\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Validate device type\n",
    "    if device is None or device.type != 'cuda':\n",
    "        logger.warning(f\"Device is {device}. Mixed precision requires CUDA. Falling back to float32.\")\n",
    "        use_amp = False\n",
    "    else:\n",
    "        use_amp = True\n",
    "        logger.info(\"Using mixed precision training with CUDA.\")\n",
    "\n",
    "    # Manually define class weights for 7 classes\n",
    "    class_weights = torch.tensor([1.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0], dtype=torch.float).to(device)\n",
    "    logger.info(f\"Class weights: {class_weights.tolist()}\")\n",
    "\n",
    "    # Loss functions\n",
    "    ce_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    supcon_criterion = SupConLoss(temperature=0.7)  # Assuming SupConLoss is defined\n",
    "    lambda_supcon = 0.4\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    # Initialize scaler\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        print('-' * 50)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0.0\n",
    "        optimizer.zero_grad()  # Clear gradients at start\n",
    "\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Skip invalid batches\n",
    "            if inputs is None or labels is None:\n",
    "                logger.debug(\"Skipping invalid batch due to None values\")\n",
    "                continue\n",
    "\n",
    "            logger.debug(f\"Batch labels type: {type(labels)}\")\n",
    "            # Handle CutMix/MixUp labels\n",
    "            if isinstance(labels, tuple):\n",
    "                labels1, labels2, lam = labels\n",
    "                labels1, labels2 = labels1.to(device), labels2.to(device)\n",
    "\n",
    "            else:\n",
    "                if not isinstance(labels, torch.Tensor):\n",
    "                    logger.error(f\"Invalid labels type: {type(labels)}, expected tensor\")\n",
    "                    continue\n",
    "                labels = labels.to(device)\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
    "                    logits, proj = model(inputs)\n",
    "                    if isinstance(labels, tuple):\n",
    "                        ce_loss = lam * ce_criterion(logits, labels1) + (1 - lam) * ce_criterion(logits, labels2)\n",
    "                    else:\n",
    "                        ce_loss = ce_criterion(logits, labels)\n",
    "                    supcon_loss = supcon_criterion(proj, labels if not isinstance(labels, tuple) else labels1)\n",
    "                    total_loss = (ce_loss + lambda_supcon * supcon_loss) / accum_steps  # Scale loss\n",
    "            else:\n",
    "                logits, proj = model(inputs)\n",
    "                if isinstance(labels, tuple):\n",
    "                    ce_loss = lam * ce_criterion(logits, labels1) + (1 - lam) * ce_criterion(logits, labels2)\n",
    "                else:\n",
    "                    ce_loss = ce_criterion(logits, labels)\n",
    "                supcon_loss = supcon_criterion(proj, labels if not isinstance(labels, tuple) else labels1)\n",
    "                total_loss = (ce_loss + lambda_supcon * supcon_loss) / accum_steps\n",
    "\n",
    "            if use_amp:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "            running_loss += total_loss.item() * inputs.size(0) * accum_steps\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            if isinstance(labels, tuple):\n",
    "                correct_preds += lam * (preds == labels1).sum().item() + (1 - lam) * (preds == labels2).sum().item()\n",
    "            else:\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0) if not isinstance(labels, tuple) else labels1.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        print(f'Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                # Skip invalid batches\n",
    "                if inputs is None or labels is None:\n",
    "                    logger.debug(\"Skipping invalid validation batch\")\n",
    "                    continue\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                if use_amp:\n",
    "                    with autocast(device_type='cuda', dtype=torch.float16, enabled=True):\n",
    "                        logits, _ = model(inputs)\n",
    "                        loss = ce_criterion(logits, labels)\n",
    "                else:\n",
    "                    logits, _ = model(inputs)\n",
    "                    loss = ce_criterion(logits, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "                correct_preds += (preds == labels).sum().item()\n",
    "                total_preds += labels.size(0)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        valid_losses.append(epoch_loss)\n",
    "        valid_accuracies.append(epoch_acc)\n",
    "        print(f'Validation Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "        if early_stopping:\n",
    "            early_stopping(epoch_loss, epoch, model.state_dict(), model_name_prefix)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"🚨 Early stopping triggered at epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        if epoch % 5 == 0 or (early_stopping and early_stopping.early_stop):\n",
    "            compute_metrics(model, val_loader, device, epoch + 1, \"val\", save_dir)  # Assuming compute_metrics is defined\n",
    "\n",
    "        torch.cuda.empty_cache()  # Clear memory after each epoch\n",
    "\n",
    "    # Load best model if early stopping was used\n",
    "    if early_stopping and early_stopping.best_epoch is not None:\n",
    "        print(f\"Loading best model weights from epoch {early_stopping.best_epoch + 1}...\")\n",
    "        best_model_path = os.path.join(save_dir, f\"{model_name_prefix}_epoch_{early_stopping.best_epoch + 1}.pth\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "    else:\n",
    "        print(\"No early stopping triggered or best epoch not set. Keeping final epoch weights.\")\n",
    "\n",
    "    plot_loss_accuracy(train_losses, valid_losses, train_accuracies, valid_accuracies, save_dir)  # Assuming plot_loss_accuracy is defined\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb26d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Testing\n",
    "def test_model(model, test_loader, device, model_name_prefix, \n",
    "               save_dir=r\"O:\\O drive\\AI\\my project\\medical image projects\\BraCs\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            with torch.amp.autocast('cuda', dtype=torch.float16, enabled=True):\n",
    "                logits, _ = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    test_accuracy = correct_preds / total_preds\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    compute_metrics(model, test_loader, device, \"test\", \"test\", save_dir)\n",
    "\n",
    "# Metrics Computation\n",
    "def compute_metrics(model, dataloader, device, epoch, split_name, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits, _ = model(inputs)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    auc_scores = {}\n",
    "    for i, class_name in enumerate(dataloader.dataset.classes):\n",
    "        auc_scores[class_name] = roc_auc_score(\n",
    "            (all_labels == i).astype(int), all_probs[:, i]\n",
    "        )\n",
    "    print(f\"{split_name.capitalize()} AUC-ROC Scores: {auc_scores}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=dataloader.dataset.classes,\n",
    "                yticklabels=dataloader.dataset.classes)\n",
    "    plt.title(f\"{split_name.capitalize()} Confusion Matrix - Epoch {epoch}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(os.path.join(save_dir, f\"{split_name}_confusion_matrix_epoch_{epoch}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, \n",
    "                                  target_names=dataloader.dataset.classes, digits=4)\n",
    "    with open(os.path.join(save_dir, f\"{split_name}_classification_report_epoch_{epoch}.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"{split_name.capitalize()} Classification Report:\\n{report}\")\n",
    "\n",
    "    return auc_scores\n",
    "\n",
    "# Plotting Loss and Accuracy\n",
    "def plot_loss_accuracy(train_losses, valid_losses, train_accuracies, valid_accuracies, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss', color='blue', marker='o')\n",
    "    plt.plot(epochs, valid_losses, label='Validation Loss', color='red', marker='o')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Training Accuracy', color='blue', marker='o')\n",
    "    plt.plot(epochs, valid_accuracies, label='Validation Accuracy', color='red', marker='o')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, 'training_validation_metrics.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b472be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7f3fac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 12:28:18,880 - INFO - Loading model: resnet18 with 7 classes on cuda\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\New folder\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:16<00:00, 2.90MB/s]\n",
      "2025-05-29 12:28:36,195 - INFO - Inserting CBAM with 64 channels\n",
      "2025-05-29 12:28:36,203 - INFO - Model resnet18 loaded successfully with 512 input features to classifier\n",
      "2025-05-29 12:28:37,469 - INFO - Using mixed precision training with CUDA.\n",
      "2025-05-29 12:28:37,469 - INFO - Class weights: [1.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training resnext50 with CBAM and Contrastive Learning for 7 classes...\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.8573, Accuracy: 0.1573\n",
      "Validation Loss: 1.8325, Accuracy: 0.2154\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_1.pth.\n",
      "Val AUC-ROC Scores: {'0_N': 0.5357670221493027, '1_PB': 0.7378514404720584, '2_UDH': 0.6690730106644791, '3_FEA': 0.48426546190995484, '4_ADH': 0.5968383017163505, '5_DCIS': 0.7213423831070889, '6_IC': 0.9539007092198581}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\alira\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.0000    0.0000    0.0000        46\n",
      "        1_PB     0.2044    0.6512    0.3111        43\n",
      "       2_UDH     0.2437    0.6304    0.3515        46\n",
      "       3_FEA     0.0000    0.0000    0.0000        49\n",
      "       4_ADH     0.1818    0.2439    0.2083        41\n",
      "      5_DCIS     0.0000    0.0000    0.0000        39\n",
      "        6_IC     0.0000    0.0000    0.0000        47\n",
      "\n",
      "    accuracy                         0.2154       311\n",
      "   macro avg     0.0900    0.2179    0.1244       311\n",
      "weighted avg     0.0883    0.2154    0.1225       311\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.7383, Accuracy: 0.1867\n",
      "Validation Loss: 1.7599, Accuracy: 0.2283\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_2.pth.\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.6452, Accuracy: 0.2187\n",
      "Validation Loss: 1.7128, Accuracy: 0.2283\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_3.pth.\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.5557, Accuracy: 0.2401\n",
      "Validation Loss: 1.6147, Accuracy: 0.2540\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_4.pth.\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.4550, Accuracy: 0.2933\n",
      "Validation Loss: 1.5201, Accuracy: 0.3408\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_5.pth.\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.3810, Accuracy: 0.3361\n",
      "Validation Loss: 1.3803, Accuracy: 0.4469\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_6.pth.\n",
      "Val AUC-ROC Scores: {'0_N': 0.8814602132895817, '1_PB': 0.7731690385282888, '2_UDH': 0.7767842493847416, '3_FEA': 0.8870540582645272, '4_ADH': 0.7410117434507679, '5_DCIS': 0.9636123680241329, '6_IC': 0.9824306898774984}\n",
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.6364    0.3043    0.4118        46\n",
      "        1_PB     0.3333    0.6512    0.4409        43\n",
      "       2_UDH     0.3770    0.5000    0.4299        46\n",
      "       3_FEA     0.9000    0.1837    0.3051        49\n",
      "       4_ADH     0.2651    0.5366    0.3548        41\n",
      "      5_DCIS     0.8000    0.1026    0.1818        39\n",
      "        6_IC     0.8478    0.8298    0.8387        47\n",
      "\n",
      "    accuracy                         0.4469       311\n",
      "   macro avg     0.5942    0.4440    0.4233       311\n",
      "weighted avg     0.6012    0.4469    0.4299       311\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.2887, Accuracy: 0.4078\n",
      "Validation Loss: 1.2498, Accuracy: 0.5241\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_7.pth.\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.2128, Accuracy: 0.4616\n",
      "Validation Loss: 1.1976, Accuracy: 0.5434\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_8.pth.\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.1496, Accuracy: 0.5038\n",
      "Validation Loss: 1.1389, Accuracy: 0.5852\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_9.pth.\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.1072, Accuracy: 0.5396\n",
      "Validation Loss: 1.1717, Accuracy: 0.5595\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.0513, Accuracy: 0.5716\n",
      "Validation Loss: 1.1289, Accuracy: 0.5691\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_11.pth.\n",
      "Val AUC-ROC Scores: {'0_N': 0.9063166529942576, '1_PB': 0.8118708781673031, '2_UDH': 0.8427399507793273, '3_FEA': 0.9035675338837825, '4_ADH': 0.7719963866305329, '5_DCIS': 0.9717194570135747, '6_IC': 0.990892972275951}\n",
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.6774    0.4565    0.5455        46\n",
      "        1_PB     0.4000    0.5581    0.4660        43\n",
      "       2_UDH     0.4237    0.5435    0.4762        46\n",
      "       3_FEA     0.6579    0.5102    0.5747        49\n",
      "       4_ADH     0.3143    0.2683    0.2895        41\n",
      "      5_DCIS     0.7209    0.7949    0.7561        39\n",
      "        6_IC     0.8889    0.8511    0.8696        47\n",
      "\n",
      "    accuracy                         0.5691       311\n",
      "   macro avg     0.5833    0.5689    0.5682       311\n",
      "weighted avg     0.5880    0.5691    0.5705       311\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 1.0215, Accuracy: 0.5806\n",
      "Validation Loss: 1.1162, Accuracy: 0.5949\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_12.pth.\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.9990, Accuracy: 0.5912\n",
      "Validation Loss: 1.0894, Accuracy: 0.6045\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_13.pth.\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.9692, Accuracy: 0.6206\n",
      "Validation Loss: 1.1363, Accuracy: 0.5691\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.9417, Accuracy: 0.6465\n",
      "Validation Loss: 1.1832, Accuracy: 0.5466\n",
      "Epoch 16/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.9241, Accuracy: 0.6453\n",
      "Validation Loss: 1.0992, Accuracy: 0.6013\n",
      "Val AUC-ROC Scores: {'0_N': 0.938884331419196, '1_PB': 0.8558660187434919, '2_UDH': 0.8433141919606235, '3_FEA': 0.9336345225112945, '4_ADH': 0.8064137308039747, '5_DCIS': 0.9770927601809956, '6_IC': 0.9933107672469375}\n",
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.6875    0.4783    0.5641        46\n",
      "        1_PB     0.3929    0.5116    0.4444        43\n",
      "       2_UDH     0.4247    0.6739    0.5210        46\n",
      "       3_FEA     0.6182    0.6939    0.6538        49\n",
      "       4_ADH     0.3750    0.0732    0.1224        41\n",
      "      5_DCIS     0.7500    0.8462    0.7952        39\n",
      "        6_IC     0.9767    0.8936    0.9333        47\n",
      "\n",
      "    accuracy                         0.6013       311\n",
      "   macro avg     0.6036    0.5958    0.5763       311\n",
      "weighted avg     0.6073    0.6013    0.5819       311\n",
      "\n",
      "Epoch 17/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.8749, Accuracy: 0.6667\n",
      "Validation Loss: 1.1386, Accuracy: 0.5916\n",
      "Epoch 18/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.8727, Accuracy: 0.6597\n",
      "Validation Loss: 1.1181, Accuracy: 0.5884\n",
      "Epoch 19/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.8513, Accuracy: 0.6831\n",
      "Validation Loss: 1.1780, Accuracy: 0.5788\n",
      "Epoch 20/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.8300, Accuracy: 0.6835\n",
      "Validation Loss: 1.1205, Accuracy: 0.5949\n",
      "Epoch 21/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7990, Accuracy: 0.7031\n",
      "Validation Loss: 1.1992, Accuracy: 0.5884\n",
      "Val AUC-ROC Scores: {'0_N': 0.9349466776045939, '1_PB': 0.849184311003124, '2_UDH': 0.8258408531583264, '3_FEA': 0.9413460040504751, '4_ADH': 0.8104787714543812, '5_DCIS': 0.9719079939668176, '6_IC': 0.9896840747904578}\n",
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.7241    0.4565    0.5600        46\n",
      "        1_PB     0.3544    0.6512    0.4590        43\n",
      "       2_UDH     0.4000    0.5652    0.4685        46\n",
      "       3_FEA     0.6600    0.6735    0.6667        49\n",
      "       4_ADH     0.5714    0.0976    0.1667        41\n",
      "      5_DCIS     0.7561    0.7949    0.7750        39\n",
      "        6_IC     1.0000    0.8511    0.9195        47\n",
      "\n",
      "    accuracy                         0.5884       311\n",
      "   macro avg     0.6380    0.5843    0.5736       311\n",
      "weighted avg     0.6405    0.5884    0.5787       311\n",
      "\n",
      "Epoch 22/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7970, Accuracy: 0.6935\n",
      "Validation Loss: 1.0978, Accuracy: 0.6399\n",
      "Epoch 23/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7997, Accuracy: 0.6889\n",
      "Validation Loss: 1.0865, Accuracy: 0.5852\n",
      "✅ Best model weights saved to O:\\O drive\\AI\\my project\\medical image projects\\BraCs\\resnet_18_contrastive_bracs_epoch_23.pth.\n",
      "Epoch 24/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7585, Accuracy: 0.7191\n",
      "Validation Loss: 1.1216, Accuracy: 0.6174\n",
      "Epoch 25/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7408, Accuracy: 0.7205\n",
      "Validation Loss: 1.1259, Accuracy: 0.6302\n",
      "Epoch 26/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7158, Accuracy: 0.7273\n",
      "Validation Loss: 1.1296, Accuracy: 0.6238\n",
      "Val AUC-ROC Scores: {'0_N': 0.9497949138638229, '1_PB': 0.8592502603262756, '2_UDH': 0.8558654634946677, '3_FEA': 0.9318429661941112, '4_ADH': 0.8105691056910569, '5_DCIS': 0.974264705882353, '6_IC': 0.9916183107672469}\n",
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.7273    0.6957    0.7111        46\n",
      "        1_PB     0.4423    0.5349    0.4842        43\n",
      "       2_UDH     0.4559    0.6739    0.5439        46\n",
      "       3_FEA     0.6531    0.6531    0.6531        49\n",
      "       4_ADH     0.3333    0.0976    0.1509        41\n",
      "      5_DCIS     0.7273    0.8205    0.7711        39\n",
      "        6_IC     0.9524    0.8511    0.8989        47\n",
      "\n",
      "    accuracy                         0.6238       311\n",
      "   macro avg     0.6131    0.6181    0.6019       311\n",
      "weighted avg     0.6181    0.6238    0.6079       311\n",
      "\n",
      "Epoch 27/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7144, Accuracy: 0.7395\n",
      "Validation Loss: 1.1327, Accuracy: 0.6109\n",
      "Epoch 28/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7131, Accuracy: 0.7385\n",
      "Validation Loss: 1.1065, Accuracy: 0.6367\n",
      "Epoch 29/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.6976, Accuracy: 0.7467\n",
      "Validation Loss: 1.1939, Accuracy: 0.5949\n",
      "Epoch 30/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.7099, Accuracy: 0.7367\n",
      "Validation Loss: 1.1392, Accuracy: 0.6109\n",
      "Epoch 31/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.6613, Accuracy: 0.7569\n",
      "Validation Loss: 1.2143, Accuracy: 0.6109\n",
      "Val AUC-ROC Scores: {'0_N': 0.9442165709598032, '1_PB': 0.8586428323498785, '2_UDH': 0.8232977850697293, '3_FEA': 0.9436828166381056, '4_ADH': 0.8196928635953026, '5_DCIS': 0.9668174962292609, '6_IC': 0.9938749194068344}\n",
      "Val Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.6444    0.6304    0.6374        46\n",
      "        1_PB     0.4118    0.6512    0.5045        43\n",
      "       2_UDH     0.4375    0.4565    0.4468        46\n",
      "       3_FEA     0.6591    0.5918    0.6237        49\n",
      "       4_ADH     0.4167    0.1220    0.1887        41\n",
      "      5_DCIS     0.7083    0.8718    0.7816        39\n",
      "        6_IC     0.9565    0.9362    0.9462        47\n",
      "\n",
      "    accuracy                         0.6109       311\n",
      "   macro avg     0.6049    0.6086    0.5898       311\n",
      "weighted avg     0.6091    0.6109    0.5943       311\n",
      "\n",
      "Epoch 32/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.6675, Accuracy: 0.7515\n",
      "Validation Loss: 1.2099, Accuracy: 0.6013\n",
      "Epoch 33/50\n",
      "--------------------------------------------------\n",
      "Training Loss: 0.6589, Accuracy: 0.7521\n",
      "Validation Loss: 1.2754, Accuracy: 0.5981\n",
      "Early stopping triggered after 10 epochs of no improvement.\n",
      "🚨 Early stopping triggered at epoch 33.\n",
      "Loading best model weights from epoch 23...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = len(train_dataset.classes)\n",
    "    \n",
    "    model = get_model(\"resnet18\", num_classes, device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.00008, weight_decay=0.008)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.8, patience=10)\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=10, verbose=True, delta=0, \n",
    "        save_dir=r\"O:\\O drive\\AI\\my project\\medical image projects\\BraCs\"\n",
    "    )\n",
    "    model_name_prefix = \"resnet_18_contrastive_bracs\"\n",
    "    \n",
    "    print(f\"Training resnext50 with CBAM and Contrastive Learning for {num_classes} classes...\")\n",
    "    model = train_and_validate(\n",
    "        model, train_loader, val_loader, optimizer, scheduler,\n",
    "        model_name_prefix, epochs=50, device=device, early_stopping=early_stopping\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e3b53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n",
      "Test Loss: 1.2268, Test Accuracy: 0.5965\n",
      "Test AUC-ROC Scores: {'0_N': 0.9420838698275644, '1_PB': 0.8069297996854778, '2_UDH': 0.8395141943222711, '3_FEA': 0.9512629573736424, '4_ADH': 0.7915130578256723, '5_DCIS': 0.8842935112189206, '6_IC': 0.9796258426115276}\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0_N     0.7463    0.6173    0.6757        81\n",
      "        1_PB     0.4157    0.4684    0.4405        79\n",
      "       2_UDH     0.4211    0.5854    0.4898        82\n",
      "       3_FEA     0.7586    0.7952    0.7765        83\n",
      "       4_ADH     0.4133    0.3924    0.4026        79\n",
      "      5_DCIS     0.6406    0.4824    0.5503        85\n",
      "        6_IC     0.9054    0.8272    0.8645        81\n",
      "\n",
      "    accuracy                         0.5965       570\n",
      "   macro avg     0.6144    0.5954    0.6000       570\n",
      "weighted avg     0.6162    0.5965    0.6013       570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Testing the model...\")\n",
    "test_model(model, test_loader, device, model_name_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3ea20",
   "metadata": {},
   "source": [
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7355407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d1eac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
